import torch

class AudioFrameWinSize:
    """
    éŸ³é¢‘æ»‘åŠ¨çª—å£å€¼è®¡ç®—èŠ‚ç‚¹
    æ”¯æŒè¾“å…¥ ANY ç±»å‹ï¼ˆå¯å…¼å®¹ AudioEncoder è¾“å‡ºï¼‰
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input_tensor": ("ANY",),
                "window_size": ("INT", {"default": 1024, "min": 1}),
                "step_size": ("INT", {"default": 512, "min": 1}),
            }
        }

    RETURN_TYPES = ("ANY",)
    FUNCTION = "compute_window"
    CATEGORY = "Audio/Utils"
    DISPLAY_NAME = "éŸ³é¢‘æ»‘åŠ¨çª—å£å€¼è®¡ç®—"

    def compute_window(self, input_tensor, window_size, step_size):
        # å…¼å®¹å¤šç§è¾“å…¥ç±»å‹
        if hasattr(input_tensor, "tensor"):
            tensor = input_tensor.tensor
        elif hasattr(input_tensor, "latents"):
            tensor = input_tensor.latents
        elif isinstance(input_tensor, torch.Tensor):
            tensor = input_tensor
        else:
            raise TypeError(f"Unsupported input type: {type(input_tensor)}")

        # ç¡®ä¿ä¸º2Då¼ é‡ [channels, samples]
        if tensor.dim() == 1:
            tensor = tensor.unsqueeze(0)

        total_len = tensor.shape[-1]
        windows = []

        for start in range(0, total_len - window_size + 1, step_size):
            end = start + window_size
            win = tensor[..., start:end]
            windows.append(win)

        if not windows:
            return (tensor,)

        stacked = torch.stack(windows, dim=0)
        return (stacked,)


# èŠ‚ç‚¹æ³¨å†Œä¿¡æ¯
NODE_CLASS_MAPPINGS = {
    "AudioFrameWinSize": AudioFrameWinSize
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AudioFrameWinSize": "ğŸ§ éŸ³é¢‘æ»‘åŠ¨çª—å£å€¼è®¡ç®—",
}
